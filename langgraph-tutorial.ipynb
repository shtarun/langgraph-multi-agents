{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to LangGraph\n",
    "\n",
    "LangGraph is a framework for creating applications using graph-based workflows. Each node represents a function or computational step, and edges define the flow between these nodes based on certain conditions.\n",
    "\n",
    "## Key Features:\n",
    "- State Management\n",
    "- Flexible Routing\n",
    "- Persistence\n",
    "- Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Overview: Text Analysis Pipeline\n",
    "\n",
    "In this tutorial, we'll demonstrate the power of LangGraph by building a multi-step text analysis pipeline. Our use case will focus on processing a given text through three key stages:\n",
    "\n",
    "1. **Text Classification**: We'll categorize the input text into predefined categories (e.g., News, Blog, Research, or Other).\n",
    "2. **Entity Extraction**: We'll identify and extract key entities such as persons, organizations, and locations from the text.\n",
    "3. **Text Summarization**: Finally, we'll generate a concise summary of the input text.\n",
    "\n",
    "This pipeline showcases how LangGraph can be used to create a modular, extensible workflow for natural language processing tasks. By the end of this tutorial, you'll understand how to construct a graph-based application that can be easily modified or expanded for various text analysis needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Libraries\n",
    "This cell imports all the necessary modules and classes for our LangGraph tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "from IPython.display import display, Image\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up API Key\n",
    "This cell loads environment variables and sets up the OpenAI API key. Make sure you have a `.env` file with your `OPENAI_API_KEY`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Text Processing Pipeline\n",
    "\n",
    "### Define State and Initialize LLM\n",
    "Here we define the State class to hold our workflow data and initialize the ChatOpenAI model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    text: str\n",
    "    classification: str\n",
    "    entities: List[str]\n",
    "    summary: str\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Node Functions\n",
    "These functions define the operations performed at each node of our graph: classification, entity extraction, and summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_node(state: State):\n",
    "    ''' Classify the text into one of the categories: News, Blog, Research, or Other '''\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"Classify the following text into one of the categories: News, Blog, Research, or Other.\\n\\nText:{text}\\n\\nCategory:\"\n",
    "    )\n",
    "    message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
    "    classification = llm.invoke([message]).content.strip()\n",
    "    return {\"classification\": classification}\n",
    "\n",
    "\n",
    "def entity_extraction_node(state: State):\n",
    "    ''' Extract all the entities (Person, Organization, Location) from the text '''\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"Extract all the entities (Person, Organization, Location) from the following text. Provide the result as a comma-separated list.\\n\\nText:{text}\\n\\nEntities:\"\n",
    "    )\n",
    "    message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
    "    entities = llm.invoke([message]).content.strip().split(\", \")\n",
    "    return {\"entities\": entities}\n",
    "\n",
    "\n",
    "def summarization_node(state: State):\n",
    "    ''' Summarize the text in one short sentence '''\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"Summarize the following text in one short sentence.\\n\\nText:{text}\\n\\nSummary:\"\n",
    "    )\n",
    "    message = HumanMessage(content=prompt.format(text=state[\"text\"]))\n",
    "    summary = llm.invoke([message]).content.strip()\n",
    "    return {\"summary\": summary}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Tools and Build Workflow\n",
    "This cell builds the StateGraph workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(State)\n",
    "\n",
    "# Add nodes to the graph\n",
    "workflow.add_node(\"classification_node\", classification_node)\n",
    "workflow.add_node(\"entity_extraction\", entity_extraction_node)\n",
    "workflow.add_node(\"summarization\", summarization_node)\n",
    "\n",
    "# Add edges to the graph\n",
    "workflow.set_entry_point(\"classification_node\") # Set the entry point of the graph\n",
    "workflow.add_edge(\"classification_node\", \"entity_extraction\")\n",
    "workflow.add_edge(\"entity_extraction\", \"summarization\")\n",
    "workflow.add_edge(\"summarization\", END)\n",
    "\n",
    "# Compile the graph\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Workflow\n",
    "This cell creates a visual representation of our workflow using Mermaid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Pipeline\n",
    "This cell runs a sample text through our pipeline and displays the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124mOpenAI has announced the GPT-4 model, which is a large multimodal model that exhibits human-level performance on various professional benchmarks. It is developed to improve the alignment and safety of AI systems.\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124madditionally, the model is designed to be more efficient and scalable than its predecessor, GPT-3. The GPT-4 model is expected to be released in the coming months and will be available to the public for research and development purposes.\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      6\u001b[0m state_input \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m: sample_text}\n\u001b[0;32m----> 7\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mapp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEntities:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentities\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1586\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m   1587\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1588\u001b[0m     config,\n\u001b[1;32m   1589\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39mstream_mode,\n\u001b[1;32m   1590\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[1;32m   1591\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[1;32m   1592\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[1;32m   1593\u001b[0m     debug\u001b[38;5;241m=\u001b[39mdebug,\n\u001b[1;32m   1594\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1595\u001b[0m ):\n\u001b[1;32m   1596\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1597\u001b[0m         latest \u001b[38;5;241m=\u001b[39m chunk\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/langgraph/pregel/__init__.py:1315\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1305\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1310\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1311\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1312\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1313\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1314\u001b[0m     ):\n\u001b[0;32m-> 1315\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1316\u001b[0m             loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m   1317\u001b[0m             timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m   1318\u001b[0m             retry_policy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mretry_policy,\n\u001b[1;32m   1319\u001b[0m             get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[1;32m   1320\u001b[0m         ):\n\u001b[1;32m   1321\u001b[0m             \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[1;32m   1322\u001b[0m             \u001b[38;5;28;01myield from\u001b[39;00m output()\n\u001b[1;32m   1323\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/langgraph/pregel/runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/langgraph/utils/runnable.py:410\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    408\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/langgraph/utils/runnable.py:184\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 184\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m, in \u001b[0;36mclassification_node\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m      4\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      5\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassify the following text into one of the categories: News, Blog, Research, or Other.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mText:\u001b[39m\u001b[38;5;132;01m{text}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCategory:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m message \u001b[38;5;241m=\u001b[39m HumanMessage(content\u001b[38;5;241m=\u001b[39mprompt\u001b[38;5;241m.\u001b[39mformat(text\u001b[38;5;241m=\u001b[39mstate[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m----> 8\u001b[0m classification \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassification\u001b[39m\u001b[38;5;124m\"\u001b[39m: classification}\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    282\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    283\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    285\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 286\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    296\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/langchain_openai/chat_models/base.py:705\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    703\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/openai/resources/chat/completions.py:815\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    814\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    821\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    823\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    824\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/openai/_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1276\u001b[0m     )\n\u001b[0;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/openai/_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/openai/_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1042\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/openai/_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/openai/_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[1;32m   1042\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[0;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/openai/_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[0;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/linuxbrew/.linuxbrew/Cellar/jupyterlab/4.2.5_1/libexec/lib/python3.12/site-packages/openai/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "OpenAI has announced the GPT-4 model, which is a large multimodal model that exhibits human-level performance on various professional benchmarks. It is developed to improve the alignment and safety of AI systems.\n",
    "additionally, the model is designed to be more efficient and scalable than its predecessor, GPT-3. The GPT-4 model is expected to be released in the coming months and will be available to the public for research and development purposes.\n",
    "\"\"\"\n",
    "\n",
    "state_input = {\"text\": sample_text}\n",
    "result = app.invoke(state_input)\n",
    "\n",
    "print(\"Classification:\", result[\"classification\"])\n",
    "print(\"\\nEntities:\", result[\"entities\"])\n",
    "print(\"\\nSummary:\", result[\"summary\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we've:\n",
    "- Explored LangGraph concepts\n",
    "- Built a text processing pipeline\n",
    "- Demonstrated LangGraph's use in data processing workflows\n",
    "- Visualized the workflow using Mermaid\n",
    "\n",
    "This example showcases how LangGraph can be used for tasks beyond conversational agents, providing a flexible framework for creating complex, graph-based workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
